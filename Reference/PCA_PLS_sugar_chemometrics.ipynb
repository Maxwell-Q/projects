{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd \n",
    "import  numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "import  seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A [Chemometrics](https://en.wikipedia.org/wiki/Chemometrics) example from Multi- and Megavariate Data Analysis by Umetrics:\n",
    "### A sugar plant in Scandinavia needs to measure the ash content of the sugar, an important quality parameter. Traditional wet-chemistry techniques are intrusive and time-consuming. The company is exploring the use of an on-line fluorescent spectroscopy method.\n",
    "### 106 fluorescence measurements (time points) were taken during the 2.5 days start-up period of the plant, resulting in 571 discrete wavelengths (or X variables). For the multivariate calibration of our model, two wet-chemistry outputs Y were measured (i.e. V1_2 and V2_2).\n",
    "### Let's take a look at the raw data. \n",
    "- The first 2 columns are metadata and can be ignored.\n",
    "- Columns V1-V571 are the spectral data that are used as input X\n",
    "- The last 2 columns V1_2 and V2_2 are the outputs Y we want to predict. Here, we show how to predict V1_2\n",
    "\n",
    "<img src=\"Fig1_Intro_data.png\"\n",
    "     style=\"float: left; margin-right: 10px; width: 700px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar = pd.read_excel('Sugar.xlsx')\n",
    "sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice based on index only the spectral input X, and plot the 106 spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar_spectra = sugar.iloc[:,2:573]\n",
    "sugar_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar_spectra.transpose().plot(legend=None,\n",
    "                              figsize=(8,6),\n",
    "                              title='Raw fluorescence data of 106 data points (lines)',\n",
    "                              xlabel='Wavelength',\n",
    "                              ylabel='Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we condense the information of each time-point or spectrum (or row) down to let's say 2-dimensions?\n",
    "### For this, we will perform PCA on the X data. This is also an EDA method. \n",
    "### Spectral inputs are pre-processed (mean-centered) and fit to a 2 principal components model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_std=False)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar_spectra_scaled = scaler.fit_transform(sugar_spectra)\n",
    "# sugar_spectra_scaled # To view the transformed data\n",
    "# sugar_spectra_scaled.mean(axis=0) # To confirm that the mean of each of the 571 variables X is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scores = pca.fit_transform(sugar_spectra_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pd = pd.DataFrame(data = pca_scores\n",
    "                         ,columns = ['PC1', 'PC2']\n",
    "                         ,index = sugar_spectra.index)\n",
    "scores_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_pd = pd.DataFrame(data = pca.components_.T\n",
    "                           ,columns = ['PC1', 'PC2']\n",
    "                           ,index = sugar_spectra.columns)\n",
    "loadings_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of the PCA transformation, dimension reduction method\n",
    "\n",
    "<img src=\"Fig_2_PCA.png\"\n",
    "     style=\"float: left; margin-right: 10px; width: 700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the scores to identify possible trends, outliers, clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_plot(scores, score_labels=None):\n",
    "    # adjusting the scores to fit in (-1,1)\n",
    "    xt = scores[:,0]\n",
    "    yt = scores[:,1]\n",
    "    scalext = 1.0/(xt.max() - xt.min())\n",
    "    scaleyt = 1.0/(yt.max() - yt.min())\n",
    "    xt_scaled = xt * scalext\n",
    "    yt_scaled = yt * scaleyt\n",
    "    \n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    for (x,y), label in zip(np.vstack((xt_scaled, yt_scaled)).T, score_labels):\n",
    "        plt.text(x, y, label, ha='center', size=11)\n",
    "        \n",
    "    plt.hlines(0, -1, 1, linestyles='solid', linewidth=3)\n",
    "    plt.vlines(0, -1, 1, linestyles='solid', linewidth=3)\n",
    "    \n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1), fontsize=20);\n",
    "    plt.ylabel(\"PC{}\".format(2), fontsize=20);\n",
    "    plt.tick_params(labelsize=16)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_plot(pca_scores[:,:2], score_labels=scores_pd.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scores show an initial transient behaviour (points 0-15 will be discarded) and an outlier (point 87 will also be discarded)\n",
    "### Let's plot the loadings to see what is the PCA picking as signal in the 2 components. Usually and for a dozen of variables we plot the loadings as vectors on the score plot. For spectroscopic data though it makes more sense to plot them as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure(figsize=(16, 6))\n",
    "sub1 = fig0.add_subplot(121)\n",
    "plt.plot(loadings_pd['PC1'],'-')\n",
    "sub1.set_xlabel('Wavelength', fontsize=20)\n",
    "sub1.set_ylabel('Loading value', fontsize=20)\n",
    "sub1.set_title('Loadings of PC #1', fontsize=20)\n",
    "sub1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "sub2 = fig0.add_subplot(122)\n",
    "plt.plot(loadings_pd['PC2'],'-')\n",
    "sub2.set_xlabel('Wavelength', fontsize=20)\n",
    "sub2.set_title('Loadings of PC #2', fontsize=20)\n",
    "sub2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first PC captures predominantly information from the wavelengths that corresponds to V161-V401.\n",
    "### PC2 captures information from the wavelengths between V50 and V150.\n",
    "### Review the explained variance of the PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "sub0 = fig.add_subplot(111)\n",
    "plt.plot(range(1,pca.n_components+1), np.cumsum(pca.explained_variance_ratio_),'b-s')\n",
    "sub0.set_xlabel('Number of components', fontsize=20)\n",
    "sub0.set_ylabel('Cumulative explained variance', fontsize=20)\n",
    "sub0.set_title('Cumulative explained variance \\n as a function of the number of components', fontsize=20)\n",
    "sub0.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the outliers, split in train/test data and fit the 3 PCs to a linear regression with the output V1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spectra_scaled = pd.DataFrame(sugar_spectra_scaled)\n",
    "X_spectra_scaled = X_spectra_scaled.iloc[15:].drop(X_spectra_scaled.index[87])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sugar.iloc[15:, -2].drop(scores_pd.index[87])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the 2 Principal Components in the **scores** table capturing 99.1% of the variance in the X data, one may think to use these two variables (or 2PCs) in a linear regression model to find the relationship between the 2 PCs and the output Y. This is called **Principal Component Regression** or [PCR](https://scikit-learn.org/dev/auto_examples/cross_decomposition/plot_pcr_vs_pls.html). Although this is not always the best solution (why? next), we strongly suggest you try that.\n",
    "### **Why** is PCR, or coupling the PCA of inputs X to a regression with the output Y, **not the best solution**?\n",
    "### Keep in mind that PCA is an unsupervised method. It does not look at the output Y. It simply finds the directions with the maximum variance. If the output Y is strongly correlated with low variance directions, then the regression will not be very efficient.\n",
    "### In contrast, **[cross decomposition or latent variable approaches](https://scikit-learn.org/stable/modules/cross_decomposition.html#cross-decomposition)** model the relationship between X and Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, use Sklearn's cross-decomposition function [PLSRegression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_spectra_scaled, y, test_size=1/2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_sklearn = PLSRegression(n_components=2, scale=False)\n",
    "pls_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_plot(pls_sklearn.x_scores_[:,:2], score_labels=scores_pd.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_sklearn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the train/test predictions vs the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = pls_sklearn.predict(X_train)\n",
    "y_test_predicted = pls_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(9, 6))\n",
    "sub5 = fig2.add_subplot(111)\n",
    "plt.scatter(y_train_predicted, y_train, s=60, marker=\"o\", edgecolors='b')\n",
    "plt.scatter(y_test_predicted, y_test, s=60, marker=\"o\", edgecolors='r')\n",
    "plt.legend(['Train set', 'Test set'])\n",
    "plt.plot([0, 0.02], [0, 0.02])\n",
    "plt.xlabel('Predicted', fontsize = 20)\n",
    "plt.ylabel('Actual', fontsize = 20)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xlim([0, 0.02])\n",
    "plt.ylim([0, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_test_predicted[:,0] - y_test\n",
    "fig3 = plt.figure(figsize=(9, 6))\n",
    "error.plot(kind='hist', title = 'Histogram of the PLS model error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average error is: %.4f' % error.mean())\n",
    "print('The average standard deviation is: %.4f' % error.std())\n",
    "print('The minimum error is: %.4f' % error.min())\n",
    "print('The maximum error is: %.4f' % error.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=(9,6))\n",
    "plt.scatter(y_test, error, s=80, marker=\"o\", facecolors='none', edgecolor='black')\n",
    "plt.hlines(0, 0, 0.02, linestyles='solid')\n",
    "plt.title('Residual errors', fontsize = 20)\n",
    "plt.xlabel('Output', fontsize = 20)\n",
    "plt.ylabel('Model error', fontsize = 20)\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xlim([0, 0.02])\n",
    "plt.ylim([-0.02, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
